{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1_fiWOzhuZ0sMC67ulG-9HJuQCdN6HRNm","timestamp":1686840247742}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Install Required Libraries:"],"metadata":{"id":"eQjVTOFd6Kpq"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8edX1hYX5_Q1","executionInfo":{"status":"ok","timestamp":1686796555306,"user_tz":-330,"elapsed":7874,"user":{"displayName":"sujitha kurup","userId":"15612511966335175667"}},"outputId":"da286999-d3d7-4018-f263-2e0879ac5125"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.2)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.9)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.1)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.1)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.7)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.2)\n"]}],"source":["!pip install nltk\n","!pip install spacy"]},{"cell_type":"markdown","source":["Import Libraries:"],"metadata":{"id":"d2Vnpn1M6JbR"}},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import string\n","import spacy"],"metadata":{"id":"970LrVpB7JXS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZuIRwIbz718g","executionInfo":{"status":"ok","timestamp":1686836447290,"user_tz":-330,"elapsed":1198,"user":{"displayName":"sujitha kurup","userId":"15612511966335175667"}},"outputId":"6dfb2bdc-c17f-4e56-f558-db5843d90f3d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["# **Tokenization:**\n","\n","Use NLTK's word_tokenize function to tokenize the text into words:"],"metadata":{"id":"c92GH-T37WzN"}},{"cell_type":"code","source":["text = \"This is an example sentence.\"\n","tokens = word_tokenize(text)\n","print(tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9gRDF4Nd7coh","executionInfo":{"status":"ok","timestamp":1686796994502,"user_tz":-330,"elapsed":689,"user":{"displayName":"sujitha kurup","userId":"15612511966335175667"}},"outputId":"6adc391f-5ac7-4ea9-84ab-88e554da1efa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['This', 'is', 'an', 'example', 'sentence', '.']\n"]}]},{"cell_type":"markdown","source":["# **Stop Word Removal:**\n","\n","Download the stop words corpus from NLTK and remove them from the tokenized text:"],"metadata":{"id":"VgXwWp898PXd"}},{"cell_type":"code","source":["nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n","print(filtered_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EoNkAu_B8Sj6","executionInfo":{"status":"ok","timestamp":1686797113102,"user_tz":-330,"elapsed":595,"user":{"displayName":"sujitha kurup","userId":"15612511966335175667"}},"outputId":"ca121809-5278-4de0-8839-57c8cb0e56a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['example', 'sentence', '.']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"markdown","source":["# **Cleaning and Normalization:**\n","\n","In the code below, we use the NLTK package for different text cleaning and normalization tasks:\n","\n","**Tokenization**: We use word_tokenize to tokenize the text into individual words.\n","\n","**Lowercasing:** We iterate over the tokens and convert each token to lowercase using the lower() method.\n","\n","**Removing stopwords:** We utilize the stopwords corpus from NLTK to get a set of common stopwords. We then remove these stopwords from the tokenized text.\n","\n","**Lemmatization**: We use the WordNetLemmatizer from NLTK to lemmatize the tokens, converting them to their base or dictionary forms.\n","\n","The clean_text function takes a text input, applies the cleaning and normalization techniques using NLTK, and returns a list of cleaned tokens.\n","\n","Feel free to customize the function based on your specific requirements and explore other NLTK functionalities for text processing and normalization."],"metadata":{"id":"zO3K7mWQ9MYU"}},{"cell_type":"code","source":["nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E2Zc7eTb-aCg","executionInfo":{"status":"ok","timestamp":1686837298807,"user_tz":-330,"elapsed":458,"user":{"displayName":"sujitha kurup","userId":"15612511966335175667"}},"outputId":"08814269-d8e2-44b0-dbc1-50f4ae18cc30"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","\n","def clean_text(text):\n","    # Tokenization\n","    tokens = word_tokenize(text)\n","\n","    # Lowercasing\n","    tokens = [token.lower() for token in tokens]\n","\n","    # Handling contractions\n","    contractions = {\n","        \"n't\": \"not\",\n","        \"'s\": \"is\",\n","        \"'re\": \"are\",\n","        # Add more contractions as needed\n","    }\n","    tokens = [contractions[token] if token in contractions else token for token in tokens]\n","\n","    # Removing stopwords and punctuation\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n","\n","    # Lemmatization\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","\n","    return tokens\n","\n","# Example usage\n","text = \"I do not like NLP. It's too complicated!\"\n","cleaned_tokens = clean_text(text)\n","print(cleaned_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"40WeqUl19N8A","executionInfo":{"status":"ok","timestamp":1686798090717,"user_tz":-330,"elapsed":509,"user":{"displayName":"sujitha kurup","userId":"15612511966335175667"}},"outputId":"ad5dda36-6170-4ff5-b1a3-019d3a73131e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['like', 'nlp', 'complicated']\n"]}]},{"cell_type":"markdown","source":["# **Lemmatization and Stemming:**\n","\n","Use spaCy library for lemmatization or NLTK's PorterStemmer for stemming:\n"],"metadata":{"id":"yeSrVRIfAWfy"}},{"cell_type":"code","source":["import spacy\n","from nltk.stem import PorterStemmer\n","\n","# Load the English language model in spaCy\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Create a spaCy Doc object by processing the input text\n","doc = nlp(\"This is an example sentence.\")\n","\n","# Extract lemmas using spaCy\n","lemmas = [token.lemma_ for token in doc]\n","print(lemmas)\n","\n","# Define tokens from the processed Doc object\n","tokens = [token.text for token in doc]\n","\n","# Create an instance of PorterStemmer from NLTK\n","stemmer = PorterStemmer()\n","\n","# Apply stemming on the tokens using PorterStemmer\n","stems = [stemmer.stem(word) for word in tokens]\n","print(stems)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OcZgQpdaAe0-","executionInfo":{"status":"ok","timestamp":1686804941641,"user_tz":-330,"elapsed":1657,"user":{"displayName":"sujitha kurup","userId":"15612511966335175667"}},"outputId":"1c16804d-a4bd-4c8f-ccce-0f3cbb253dd2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['this', 'be', 'an', 'example', 'sentence', '.']\n","['thi', 'is', 'an', 'exampl', 'sentenc', '.']\n"]}]},{"cell_type":"markdown","source":["The first print statement outputs the lemmas extracted using spaCy:\n","\n","['this', 'be', 'an', 'example', 'sentence', '.'].\n","\n","The lemmas are obtained using the lemma_ attribute of each token in the doc object. These lemmas represent the base or dictionary form of the corresponding words in the input text.\n","\n","\n","The second print statement outputs the stems obtained using the PorterStemmer:\n","\n"," ['thi', 'is', 'an', 'exampl', 'sentenc', '.'].\n","\n"," The stems are obtained by applying the stem() method of the PorterStemmer to each token in the tokens list. The stems represent the root form of the corresponding words in the input text, derived using the Porter stemming algorithm.\n","\n"],"metadata":{"id":"kZEr2YnUaNzM"}}]}